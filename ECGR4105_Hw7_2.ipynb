{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcarroth-dev/KarltonCarrothers_ML/blob/main/ECGR4105_Hw7_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WMsvA_bRSpiK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "zbe-cS-mSvtL",
        "outputId": "f6666cf1-055a-4373-e8b6-f7158f0fa041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300, Loss: 1.8290543197975744, Training Time: 21.17 seconds\n",
            "Epoch 11/300, Loss: 0.6817077270249272, Training Time: 20.64 seconds\n",
            "Epoch 21/300, Loss: 0.1791041426341552, Training Time: 20.72 seconds\n",
            "Epoch 31/300, Loss: 0.028237364527023853, Training Time: 20.82 seconds\n",
            "Epoch 41/300, Loss: 0.011113108452701288, Training Time: 20.74 seconds\n",
            "Epoch 51/300, Loss: 0.008055606189414935, Training Time: 20.81 seconds\n",
            "Epoch 61/300, Loss: 0.005191369487605084, Training Time: 20.91 seconds\n",
            "Epoch 71/300, Loss: 0.009978081582693497, Training Time: 20.73 seconds\n",
            "Epoch 81/300, Loss: 0.0028142215434850083, Training Time: 20.68 seconds\n",
            "Epoch 91/300, Loss: 0.002931999276837875, Training Time: 20.73 seconds\n",
            "Epoch 101/300, Loss: 0.002171738959928972, Training Time: 20.63 seconds\n",
            "Epoch 111/300, Loss: 0.003996128597229188, Training Time: 20.79 seconds\n",
            "Epoch 121/300, Loss: 0.00800091037089797, Training Time: 20.80 seconds\n",
            "Epoch 131/300, Loss: 0.0012009977246374142, Training Time: 21.04 seconds\n",
            "Epoch 141/300, Loss: 0.0013738609026418404, Training Time: 20.59 seconds\n",
            "Epoch 151/300, Loss: 0.0010628152835813155, Training Time: 20.75 seconds\n",
            "Epoch 161/300, Loss: 0.0012953701960660554, Training Time: 20.78 seconds\n",
            "Epoch 171/300, Loss: 0.002947020652222202, Training Time: 20.77 seconds\n",
            "Epoch 181/300, Loss: 0.0012251457944988746, Training Time: 20.70 seconds\n",
            "Epoch 191/300, Loss: 0.008481269103044387, Training Time: 20.64 seconds\n",
            "Epoch 201/300, Loss: 0.002299582804827129, Training Time: 20.68 seconds\n",
            "Epoch 211/300, Loss: 0.0009572119889081941, Training Time: 20.62 seconds\n",
            "Epoch 221/300, Loss: 0.0013847003149928178, Training Time: 20.85 seconds\n",
            "Epoch 231/300, Loss: 0.005423326670681439, Training Time: 20.65 seconds\n",
            "Epoch 241/300, Loss: 0.0014046864865859554, Training Time: 20.72 seconds\n",
            "Epoch 251/300, Loss: 0.0010633300747861579, Training Time: 20.55 seconds\n",
            "Epoch 261/300, Loss: 0.0010821904750283906, Training Time: 20.74 seconds\n",
            "Epoch 271/300, Loss: 0.0009832814471861136, Training Time: 20.72 seconds\n",
            "Epoch 281/300, Loss: 0.0006727490200014437, Training Time: 20.90 seconds\n",
            "Epoch 291/300, Loss: 0.000610931363972084, Training Time: 20.65 seconds\n",
            "Total Training Time: 6219.20 seconds\n",
            "Final Test Accuracy: 66.42%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'precision_score' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1378192753.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;31m# Calculate and print F1 score, precision, and recall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'precision_score' is not defined"
          ]
        }
      ],
      "source": [
        "# Load CIFAR-10 dataset to calculate mean and std\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Calculate mean and std\n",
        "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
        "mean = imgs.view(3, -1).mean(dim=1)\n",
        "std = imgs.view(3, -1).std(dim=1)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Load Normalized CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define Convolutional Neural Network\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Convolution Layer\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Convolutional Layer\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "#ResNet-10 model\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10, n_chans1=64):\n",
        "        super(ResNet10, self).__init__()\n",
        "        self.in_channels = n_chans1\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(n_chans1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer1 = self._make_layer(block, n_chans1, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, n_chans1 * 2, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, n_chans1 * 4, num_blocks[2], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(n_chans1 * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the ResNet-10 model with ResNet blocks\n",
        "model = ResNet10(CNN, [4, 3, 3]).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Model Training\n",
        "num_epochs = 300\n",
        "total_start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
        "\n",
        "total_end_time = time.time()\n",
        "total_training_time = total_end_time - total_start_time\n",
        "print(f'Total Training Time: {total_training_time:.2f} seconds')\n",
        "\n",
        "# Model Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "all_predicted = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Final Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print F1 score, precision, and recall\n",
        "f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
        "precision = precision_score(all_labels, all_predicted, average='weighted')\n",
        "recall = recall_score(all_labels, all_predicted, average='weighted')\n",
        "\n",
        "print(f'Final F1 Score: {f1:.4f}')\n",
        "print(f'Final Precision: {precision:.4f}')\n",
        "print(f'Final Recall: {recall:.4f}')\n",
        "\n",
        "# Calculate and print confusion matrix at the end\n",
        "cm = confusion_matrix(all_labels, all_predicted)\n",
        "print('Final Confusion Matrix:')\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ2LiU1zb0Mt",
        "outputId": "87d99735-3b3f-43cb-9c6c-b7cc6d6c60c1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final F1 Score: 0.6740\n",
            "Final Precision: 0.7113\n",
            "Final Recall: 0.6642\n",
            "Final Confusion Matrix:\n",
            "[[654  34  37  21   3 126  10  43  40  32]\n",
            " [  9 830   4   9   0  72   4   5   6  61]\n",
            " [ 46   4 539  71  37 188  59  40  10   6]\n",
            " [  2   1  56 523  22 322  45  21   4   4]\n",
            " [ 22   4 106 102 425 144  70 118   4   5]\n",
            " [  2   0  28 158  34 735  11  27   1   4]\n",
            " [  4   2  37 100  16 105 727   4   1   4]\n",
            " [  6   1  18  33  22 202   2 706   0  10]\n",
            " [ 68  41  10  22   0 102   4   7 712  34]\n",
            " [ 16  71   4  15   1  78   3  12   9 791]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score\n",
        "f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
        "precision = precision_score(all_labels, all_predicted, average='weighted')\n",
        "recall = recall_score(all_labels, all_predicted, average='weighted')\n",
        "\n",
        "print(f'Final F1 Score: {f1:.4f}')\n",
        "print(f'Final Precision: {precision:.4f}')\n",
        "print(f'Final Recall: {recall:.4f}')\n",
        "\n",
        "# Calculate and print confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predicted)\n",
        "print('Final Confusion Matrix:')\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_NxCqTu1Kv0",
        "outputId": "afa18fba-164f-49f2-b2c8-22640288b318"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final F1 Score: 0.6740\n",
            "Final Precision: 0.7113\n",
            "Final Recall: 0.6642\n",
            "Final Confusion Matrix:\n",
            "[[654  34  37  21   3 126  10  43  40  32]\n",
            " [  9 830   4   9   0  72   4   5   6  61]\n",
            " [ 46   4 539  71  37 188  59  40  10   6]\n",
            " [  2   1  56 523  22 322  45  21   4   4]\n",
            " [ 22   4 106 102 425 144  70 118   4   5]\n",
            " [  2   0  28 158  34 735  11  27   1   4]\n",
            " [  4   2  37 100  16 105 727   4   1   4]\n",
            " [  6   1  18  33  22 202   2 706   0  10]\n",
            " [ 68  41  10  22   0 102   4   7 712  34]\n",
            " [ 16  71   4  15   1  78   3  12   9 791]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Calculate mean and std\n",
        "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
        "mean = imgs.view(3, -1).mean(dim=1)\n",
        "std = imgs.view(3, -1).std(dim=1)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformation with calculated mean and std\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Load Normalized CIFAR-10\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define Convolutional Neural Network\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Convolutional Layer\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Convolutional Layer\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define ResNet-10 model\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10, n_chans1=64):\n",
        "        super(ResNet10, self).__init__()\n",
        "        self.in_channels = n_chans1\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(n_chans1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer1 = self._make_layer(block, n_chans1, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, n_chans1 * 2, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, n_chans1 * 4, num_blocks[2], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(n_chans1 * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the ResNet-10 model with ResNet blocks\n",
        "model = ResNet10(CNN, [4, 3, 3]).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "\n",
        "# Model Training\n",
        "num_epochs = 300\n",
        "total_start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
        "\n",
        "total_end_time = time.time()\n",
        "total_training_time = total_end_time - total_start_time\n",
        "print(f'Total Training Time: {total_training_time:.2f} seconds')\n",
        "\n",
        "# Model Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "all_predicted = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Final Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Calculate and print F1 score, precision, and recall\n",
        "f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
        "precision = precision_score(all_labels, all_predicted, average='weighted')\n",
        "recall = recall_score(all_labels, all_predicted, average='weighted')\n",
        "\n",
        "print(f'Final F1 Score: {f1:.4f}')\n",
        "print(f'Final Precision: {precision:.4f}')\n",
        "print(f'Final Recall: {recall:.4f}')\n",
        "\n",
        "# Calculate and print confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predicted)\n",
        "print('Final Confusion Matrix:')\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMAYyWGd1UBU",
        "outputId": "2e9743f7-d455-4f50-f52c-50368b8c661c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300, Loss: 1.8370715245566405, Training Time: 21.05 seconds\n",
            "Epoch 11/300, Loss: 0.6849995207451188, Training Time: 20.81 seconds\n",
            "Epoch 21/300, Loss: 0.18762281314110207, Training Time: 20.83 seconds\n",
            "Epoch 31/300, Loss: 0.03068528662238013, Training Time: 20.96 seconds\n",
            "Epoch 41/300, Loss: 0.01102988062845662, Training Time: 20.87 seconds\n",
            "Epoch 51/300, Loss: 0.006886657848156265, Training Time: 20.92 seconds\n",
            "Epoch 61/300, Loss: 0.004378930031595266, Training Time: 20.67 seconds\n",
            "Epoch 71/300, Loss: 0.004870077243635712, Training Time: 20.86 seconds\n",
            "Epoch 81/300, Loss: 0.004436143251169292, Training Time: 20.80 seconds\n",
            "Epoch 91/300, Loss: 0.002417404338328318, Training Time: 20.88 seconds\n",
            "Epoch 101/300, Loss: 0.002389681308031263, Training Time: 20.87 seconds\n",
            "Epoch 111/300, Loss: 0.002040942951054622, Training Time: 20.85 seconds\n",
            "Epoch 121/300, Loss: 0.018811908250069006, Training Time: 20.89 seconds\n",
            "Epoch 131/300, Loss: 0.002922769788841126, Training Time: 20.74 seconds\n",
            "Epoch 141/300, Loss: 0.004057358460561277, Training Time: 20.72 seconds\n",
            "Epoch 151/300, Loss: 0.002405726996301364, Training Time: 20.65 seconds\n",
            "Epoch 161/300, Loss: 0.0020080999112994675, Training Time: 21.39 seconds\n",
            "Epoch 171/300, Loss: 0.001992369880197127, Training Time: 20.77 seconds\n",
            "Epoch 181/300, Loss: 0.001779789418910149, Training Time: 20.84 seconds\n",
            "Epoch 191/300, Loss: 0.0016211323436576507, Training Time: 20.65 seconds\n",
            "Epoch 201/300, Loss: 0.0014090775404042978, Training Time: 20.83 seconds\n",
            "Epoch 211/300, Loss: 0.0014294664317201894, Training Time: 20.81 seconds\n",
            "Epoch 221/300, Loss: 0.0022825509671668483, Training Time: 21.16 seconds\n",
            "Epoch 231/300, Loss: 0.0010603970226385425, Training Time: 20.84 seconds\n",
            "Epoch 241/300, Loss: 0.0016586453068780814, Training Time: 20.72 seconds\n",
            "Epoch 251/300, Loss: 0.003710596768051217, Training Time: 20.84 seconds\n",
            "Epoch 261/300, Loss: 0.14851712213877988, Training Time: 20.76 seconds\n",
            "Epoch 271/300, Loss: 0.0030373685596221368, Training Time: 20.81 seconds\n",
            "Epoch 281/300, Loss: 0.0018760234555851817, Training Time: 20.72 seconds\n",
            "Epoch 291/300, Loss: 0.001254029926875799, Training Time: 20.84 seconds\n",
            "Total Training Time: 6245.51 seconds\n",
            "Final Test Accuracy: 45.39%\n",
            "Final F1 Score: 0.4397\n",
            "Final Precision: 0.6727\n",
            "Final Recall: 0.4539\n",
            "Final Confusion Matrix:\n",
            "[[377   0  51  17  37   4 372   0 124  18]\n",
            " [  8  50   1   9   0   1 697   0 145  89]\n",
            " [ 17   0 369  67  58  22 441   2  22   2]\n",
            " [  1   0  19 434  27  48 445   0  24   2]\n",
            " [  3   0  33  62 558  10 298   0  35   1]\n",
            " [  4   0  22 220  35 341 365   1   8   4]\n",
            " [  0   0   6  26   3   2 956   0   7   0]\n",
            " [ 20   0  24 122 329  59 280  92  41  33]\n",
            " [ 12   0  12  14   9   1 167   0 780   5]\n",
            " [  7   1   0  20   6   2 212   0 170 582]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Calculate mean and std\n",
        "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
        "mean = imgs.view(3, -1).mean(dim=1)\n",
        "std = imgs.view(3, -1).std(dim=1)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformation with calculated mean and std\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Load Normalized CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define Convolutional Neural Network\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, dropout_prob=0.3):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_prob)\n",
        "\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.dropout2 = nn.Dropout(dropout_prob)\n",
        "\n",
        "        # Shortcut connection\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.Dropout(dropout_prob)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.dropout1(out)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = self.dropout2(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define ResNet-10 model\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10, n_chans1=64, dropout_prob=0.3):\n",
        "        super(ResNet10, self).__init__()\n",
        "        self.in_channels = n_chans1\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(n_chans1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer1 = self._make_layer(block, n_chans1, num_blocks[0], stride=1, dropout_prob=dropout_prob)\n",
        "        self.layer2 = self._make_layer(block, n_chans1 * 2, num_blocks[1], stride=2, dropout_prob=dropout_prob)\n",
        "        self.layer3 = self._make_layer(block, n_chans1 * 4, num_blocks[2], stride=2, dropout_prob=dropout_prob)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(n_chans1 * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride, dropout_prob):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride, dropout_prob))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the ResNet-10 model with ResNet blocks\n",
        "model = ResNet10(CNN, [4, 3, 3], dropout_prob=0.3).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Model Training\n",
        "num_epochs = 300\n",
        "total_start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
        "\n",
        "total_end_time = time.time()\n",
        "total_training_time = total_end_time - total_start_time\n",
        "print(f'Total Training Time: {total_training_time:.2f} seconds')\n",
        "\n",
        "# Model Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "all_predicted = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Final Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Calculate and print F1 score, precision, and recall\n",
        "f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
        "precision = precision_score(all_labels, all_predicted, average='weighted')\n",
        "recall = recall_score(all_labels, all_predicted, average='weighted')\n",
        "\n",
        "print(f'Final F1 Score: {f1:.4f}')\n",
        "print(f'Final Precision: {precision:.4f}')\n",
        "print(f'Final Recall: {recall:.4f}')\n",
        "\n",
        "# Calculate and print confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predicted)\n",
        "print('Final Confusion Matrix:')\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PW5MA6z1bn-",
        "outputId": "68bf6122-fce4-4d8b-f3fd-ebcfa60cf5ce"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300, Loss: 2.1929081071673147, Training Time: 21.75 seconds\n",
            "Epoch 11/300, Loss: 1.4337500507569374, Training Time: 21.77 seconds\n",
            "Epoch 21/300, Loss: 1.1377560472701822, Training Time: 21.82 seconds\n",
            "Epoch 31/300, Loss: 0.9706312327281289, Training Time: 22.10 seconds\n",
            "Epoch 41/300, Loss: 0.8682814081154211, Training Time: 21.73 seconds\n",
            "Epoch 51/300, Loss: 0.7951992017686215, Training Time: 21.65 seconds\n",
            "Epoch 61/300, Loss: 0.7336129161250561, Training Time: 21.66 seconds\n",
            "Epoch 71/300, Loss: 0.6776345964054318, Training Time: 21.77 seconds\n",
            "Epoch 81/300, Loss: 0.6303484962724358, Training Time: 21.79 seconds\n",
            "Epoch 91/300, Loss: 0.5927398870591922, Training Time: 21.74 seconds\n",
            "Epoch 101/300, Loss: 0.5545790942047563, Training Time: 21.82 seconds\n",
            "Epoch 111/300, Loss: 0.524302867024451, Training Time: 21.81 seconds\n",
            "Epoch 121/300, Loss: 0.4903403527443976, Training Time: 21.47 seconds\n",
            "Epoch 131/300, Loss: 0.46865252757926124, Training Time: 22.08 seconds\n",
            "Epoch 141/300, Loss: 0.4423105137046341, Training Time: 21.70 seconds\n",
            "Epoch 151/300, Loss: 0.42767943807727543, Training Time: 21.56 seconds\n",
            "Epoch 161/300, Loss: 0.4058214533130836, Training Time: 21.54 seconds\n",
            "Epoch 171/300, Loss: 0.38499952965151624, Training Time: 21.50 seconds\n",
            "Epoch 181/300, Loss: 0.3729434494700883, Training Time: 21.61 seconds\n",
            "Epoch 191/300, Loss: 0.36024282952708664, Training Time: 21.61 seconds\n",
            "Epoch 201/300, Loss: 0.34095553389709926, Training Time: 21.67 seconds\n",
            "Epoch 211/300, Loss: 0.33102865060767556, Training Time: 21.68 seconds\n",
            "Epoch 221/300, Loss: 0.31655174602404273, Training Time: 21.50 seconds\n",
            "Epoch 231/300, Loss: 0.30162441477065194, Training Time: 21.45 seconds\n",
            "Epoch 241/300, Loss: 0.29742623002404145, Training Time: 21.80 seconds\n",
            "Epoch 251/300, Loss: 0.28245794202398766, Training Time: 21.71 seconds\n",
            "Epoch 261/300, Loss: 0.2716959242701835, Training Time: 21.71 seconds\n",
            "Epoch 271/300, Loss: 0.2624660219968585, Training Time: 21.43 seconds\n",
            "Epoch 281/300, Loss: 0.2526253240892802, Training Time: 21.86 seconds\n",
            "Epoch 291/300, Loss: 0.24365447900827278, Training Time: 22.12 seconds\n",
            "Total Training Time: 6509.10 seconds\n",
            "Final Test Accuracy: 85.59%\n",
            "Final F1 Score: 0.8546\n",
            "Final Precision: 0.8605\n",
            "Final Recall: 0.8559\n",
            "Final Confusion Matrix:\n",
            "[[780   1  43   4  21   1  17   5  97  31]\n",
            " [  6 843   0   1   2   3  10   1  31 103]\n",
            " [ 22   0 842  18  34  29  31  10  10   4]\n",
            " [  8   0  52 648  51 146  52  17  16  10]\n",
            " [  4   0  41  17 896  14  16   9   3   0]\n",
            " [  4   1  28  58  40 838  18  10   2   1]\n",
            " [  5   0  22  13  22   9 922   1   4   2]\n",
            " [  6   0  19   8  39  39   2 877   7   3]\n",
            " [  9   3   1   3   3   1   4   1 966   9]\n",
            " [  6  12   2   4   3   0   3   1  22 947]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset to calculate mean and std\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Calculate mean and std\n",
        "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
        "mean = imgs.view(3, -1).mean(dim=1)\n",
        "std = imgs.view(3, -1).std(dim=1)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformation with calculated mean and std\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset with normalization\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define Convolutional Neural Network\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Shortcut connection\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define ResNet-10 model\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10, n_chans1=64):\n",
        "        super(ResNet10, self).__init__()\n",
        "        self.in_channels = n_chans1\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(n_chans1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer1 = self._make_layer(block, n_chans1, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, n_chans1 * 2, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, n_chans1 * 4, num_blocks[2], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(n_chans1 * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the ResNet-10 model with ResNet blocks and batch normalization\n",
        "model = ResNet10(CNN, [4, 3, 3]).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Model Training\n",
        "num_epochs = 300\n",
        "total_start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
        "\n",
        "total_end_time = time.time()\n",
        "total_training_time = total_end_time - total_start_time\n",
        "print(f'Total Training Time: {total_training_time:.2f} seconds')\n",
        "\n",
        "# Model Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "all_predicted = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Final Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Calculate and print F1 score, precision, and recall\n",
        "f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
        "precision = precision_score(all_labels, all_predicted, average='weighted')\n",
        "recall = recall_score(all_labels, all_predicted, average='weighted')\n",
        "\n",
        "print(f'Final F1 Score: {f1:.4f}')\n",
        "print(f'Final Precision: {precision:.4f}')\n",
        "print(f'Final Recall: {recall:.4f}')\n",
        "\n",
        "# Calculate and print confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predicted)\n",
        "print('Final Confusion Matrix:')\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5sleaGn1nm4",
        "outputId": "4bb65a40-a744-48a2-dd71-f5339a0d3dfa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300, Loss: 1.8129211501087374, Training Time: 20.64 seconds\n",
            "Epoch 11/300, Loss: 0.701896849343234, Training Time: 20.64 seconds\n",
            "Epoch 21/300, Loss: 0.2185732681504296, Training Time: 20.70 seconds\n",
            "Epoch 31/300, Loss: 0.033309594931704996, Training Time: 20.71 seconds\n",
            "Epoch 41/300, Loss: 0.011184577406753245, Training Time: 20.68 seconds\n",
            "Epoch 51/300, Loss: 0.008552617130829665, Training Time: 20.59 seconds\n",
            "Epoch 61/300, Loss: 0.00843371897946288, Training Time: 20.62 seconds\n",
            "Epoch 71/300, Loss: 0.0033418990282640766, Training Time: 20.73 seconds\n",
            "Epoch 81/300, Loss: 0.0034298691896351334, Training Time: 20.61 seconds\n",
            "Epoch 91/300, Loss: 0.003717368664292921, Training Time: 20.54 seconds\n",
            "Epoch 101/300, Loss: 0.0023898073341673634, Training Time: 20.58 seconds\n",
            "Epoch 111/300, Loss: 0.0054902860824012165, Training Time: 20.65 seconds\n",
            "Epoch 121/300, Loss: 0.0025110696265457587, Training Time: 20.97 seconds\n",
            "Epoch 131/300, Loss: 0.001742886721666115, Training Time: 20.70 seconds\n",
            "Epoch 141/300, Loss: 0.003292038845584777, Training Time: 20.65 seconds\n",
            "Epoch 151/300, Loss: 0.00398920818879281, Training Time: 20.86 seconds\n",
            "Epoch 161/300, Loss: 0.0017351846745208828, Training Time: 20.78 seconds\n",
            "Epoch 171/300, Loss: 0.0016817839081471194, Training Time: 20.83 seconds\n",
            "Epoch 181/300, Loss: 0.0012258995133672442, Training Time: 20.78 seconds\n",
            "Epoch 191/300, Loss: 0.0011150304203057456, Training Time: 20.57 seconds\n",
            "Epoch 201/300, Loss: 0.0008731509327626182, Training Time: 20.74 seconds\n",
            "Epoch 211/300, Loss: 0.002678142708053043, Training Time: 20.83 seconds\n",
            "Epoch 221/300, Loss: 0.002271874816558274, Training Time: 20.63 seconds\n",
            "Epoch 231/300, Loss: 0.0011485222865284905, Training Time: 21.00 seconds\n",
            "Epoch 241/300, Loss: 0.001275004611196219, Training Time: 20.82 seconds\n",
            "Epoch 251/300, Loss: 0.0007761923150940831, Training Time: 20.79 seconds\n",
            "Epoch 261/300, Loss: 0.0005626890193999988, Training Time: 20.81 seconds\n",
            "Epoch 271/300, Loss: 0.0009154016829516409, Training Time: 20.75 seconds\n",
            "Epoch 281/300, Loss: 0.000552152447026621, Training Time: 20.66 seconds\n",
            "Epoch 291/300, Loss: 0.0016535262075799402, Training Time: 20.61 seconds\n",
            "Total Training Time: 6215.53 seconds\n",
            "Final Test Accuracy: 72.03%\n",
            "Final F1 Score: 0.7184\n",
            "Final Precision: 0.7176\n",
            "Final Recall: 0.7203\n",
            "Final Confusion Matrix:\n",
            "[[781  24  45  16  16   3  14  12  60  29]\n",
            " [ 23 879   4   2   3   1   7   8  19  54]\n",
            " [ 65   3 602  61  74  48  97  33   9   8]\n",
            " [ 34  13  86 507  63 156  85  28  11  17]\n",
            " [ 29   6  66  61 641  35  69  77  10   6]\n",
            " [ 12   9  47 158  59 612  32  56   4  11]\n",
            " [ 14   6  51  70  36  20 781   8   8   6]\n",
            " [ 22  10  32  35  66  64  10 732   5  24]\n",
            " [ 64  25   9   6   9   4   7   9 847  20]\n",
            " [ 29  91   3  11   4   1   5   9  26 821]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP1xw6z6to/CzZNpDE8Zi2a",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}